{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53bd957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples siap evaluasi: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from src.tag.src.answer_generator import generate_answer\n",
    "\n",
    "# === Load dan filter dataset ===\n",
    "DATASET_PATH = os.path.join(\"data\", \"Dataset Testing 2.xlsx\")\n",
    "df = pd.read_excel(DATASET_PATH)\n",
    "valid_df = df[df[\"is_valid\"]]\n",
    "sampled_df = valid_df.sample(n=min(20, len(valid_df)), random_state=42)\n",
    "\n",
    "# === Hasil akhir: list of dict yang siap evaluasi ===\n",
    "samples = []\n",
    "\n",
    "for i, row in sampled_df.iterrows():\n",
    "    question = str(row[\"user_input\"])\n",
    "    ground_truth = str(row[\"reference\"])\n",
    "\n",
    "    try:\n",
    "        contexts = literal_eval(row[\"reference_contexts_2\"])\n",
    "        if not isinstance(contexts, list): continue\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Siapkan input tabular untuk generator\n",
    "    columns = [\"teks\"]\n",
    "    rows = [[ctx] for ctx in contexts]\n",
    "\n",
    "    # Generate jawaban dari sistemmu\n",
    "    answer = generate_answer(columns, rows, question, mode=\"few-shot\", llm_mode=\"api\")\n",
    "\n",
    "    samples.append({\n",
    "        \"user_input\": question,\n",
    "        \"retrieved_contexts\": contexts,\n",
    "        \"response\": answer,\n",
    "        \"reference\": ground_truth\n",
    "    })\n",
    "\n",
    "# Simpan untuk dipakai ulang (opsional)\n",
    "import json\n",
    "with open(\"generated_samples.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(samples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Total samples siap evaluasi: {len(samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcfa1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting nltk (from rouge-score)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in d:\\code\\.venv\\lib\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in d:\\code\\.venv\\lib\\site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in d:\\code\\.venv\\lib\\site-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\code\\.venv\\lib\\site-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\code\\.venv\\lib\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\code\\.venv\\lib\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\code\\.venv\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml): started\n",
      "  Building wheel for rouge-score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=1efe552112e34ab0bc7e3bacf7465909c628d9d2329df43a772ccc8962cf536c\n",
      "  Stored in directory: c:\\users\\ilham\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, nltk, rouge-score\n",
      "\n",
      "   ---------------------------------------- 0/3 [absl-py]\n",
      "   ---------------------------------------- 0/3 [absl-py]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   ------------- -------------------------- 1/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [rouge-score]\n",
      "   ---------------------------------------- 3/3 [rouge-score]\n",
      "\n",
      "Successfully installed absl-py-2.3.0 nltk-3.9.1 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total sample valid untuk evaluasi: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Mengevaluasi batch 1 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:14<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.4993, 'rouge1_recall(mode=recall)': 0.6964, 'rouge1_fmeasure(mode=fmeasure)': 0.4970, 'rougeL_precision(mode=precision)': 0.4454, 'rougeL_recall(mode=recall)': 0.6596, 'rougeL_fmeasure(mode=fmeasure)': 0.4560, 'answer_relevancy': 0.3646, 'faithfulness': 1.0000}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 2 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:12<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.4461, 'rouge1_recall(mode=recall)': 0.7084, 'rouge1_fmeasure(mode=fmeasure)': 0.5056, 'rougeL_precision(mode=precision)': 0.3800, 'rougeL_recall(mode=recall)': 0.5942, 'rougeL_fmeasure(mode=fmeasure)': 0.4301, 'answer_relevancy': 0.6904, 'faithfulness': 1.0000}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 3 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:11<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.4744, 'rouge1_recall(mode=recall)': 0.3203, 'rouge1_fmeasure(mode=fmeasure)': 0.3692, 'rougeL_precision(mode=precision)': 0.4225, 'rougeL_recall(mode=recall)': 0.2764, 'rougeL_fmeasure(mode=fmeasure)': 0.3238, 'answer_relevancy': 0.1882, 'faithfulness': 1.0000}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 4 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|████████▊ | 21/24 [00:02<00:00,  7.90it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "].\n",
      "Evaluating:  92%|█████████▏| 22/24 [00:05<00:00,  2.55it/s]WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 20\n",
      "}\n",
      "].\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.0-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "].\n",
      "Evaluating: 100%|██████████| 24/24 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.3574, 'rouge1_recall(mode=recall)': 0.8952, 'rouge1_fmeasure(mode=fmeasure)': 0.4911, 'rougeL_precision(mode=precision)': 0.3381, 'rougeL_recall(mode=recall)': 0.8514, 'rougeL_fmeasure(mode=fmeasure)': 0.4651, 'answer_relevancy': 0.3900, 'faithfulness': 0.9630}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 5 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:22<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.5501, 'rouge1_recall(mode=recall)': 0.9861, 'rouge1_fmeasure(mode=fmeasure)': 0.6740, 'rougeL_precision(mode=precision)': 0.5464, 'rougeL_recall(mode=recall)': 0.9807, 'rougeL_fmeasure(mode=fmeasure)': 0.6697, 'answer_relevancy': 0.3562, 'faithfulness': 1.0000}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 6 (3 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 24/24 [00:19<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.3928, 'rouge1_recall(mode=recall)': 0.7281, 'rouge1_fmeasure(mode=fmeasure)': 0.4746, 'rougeL_precision(mode=precision)': 0.3895, 'rougeL_recall(mode=recall)': 0.7126, 'rougeL_fmeasure(mode=fmeasure)': 0.4693, 'answer_relevancy': 0.2301, 'faithfulness': 1.0000}\n",
      "⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\n",
      "\n",
      "\n",
      "🧪 Mengevaluasi batch 7 (2 soal)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 16/16 [00:20<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1_precision(mode=precision)': 0.4095, 'rouge1_recall(mode=recall)': 0.8321, 'rouge1_fmeasure(mode=fmeasure)': 0.5402, 'rougeL_precision(mode=precision)': 0.3688, 'rougeL_recall(mode=recall)': 0.7577, 'rougeL_fmeasure(mode=fmeasure)': 0.4883, 'answer_relevancy': 0.2630, 'faithfulness': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Tambahkan path agar modul src bisa ditemukan\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from src.tag.evaluation.eval_metrics import evaluate_text_generation\n",
    "from ragas.dataset_schema import EvaluationDataset\n",
    "from src.tag.src.text2sqlchain_zero import init_llm\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# === Load hasil generate_answer dari file JSON ===\n",
    "with open(\"generated_samples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_samples = json.load(f)\n",
    "\n",
    "# === Bersihkan dan ubah field agar cocok dengan EvaluationDataset ===\n",
    "samples = []\n",
    "for s in raw_samples:\n",
    "    try:\n",
    "        question = str(s[\"user_input\"])\n",
    "        answer = str(s[\"response\"])\n",
    "        ground_truth = str(s[\"reference\"])\n",
    "        contexts = s[\"retrieved_contexts\"]\n",
    "        \n",
    "        if not isinstance(contexts, list) or not all(isinstance(c, str) for c in contexts):\n",
    "            continue\n",
    "\n",
    "        samples.append({\n",
    "            \"user_input\": question,\n",
    "            \"response\": answer,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"reference\": ground_truth\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error pada sample: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"✅ Total sample valid untuk evaluasi: {len(samples)}\")\n",
    "\n",
    "# === Inisialisasi LLM dan Embedding ===\n",
    "llm_model = init_llm(mode=\"gemini\")  # sesuai mode generate_answer\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# === Evaluasi per 3 soal ===\n",
    "batch_size = 3\n",
    "for i in range(0, len(samples), batch_size):\n",
    "    batch_samples = samples[i:i+batch_size]\n",
    "    dataset = EvaluationDataset.from_list(batch_samples)\n",
    "\n",
    "    print(f\"\\n🧪 Mengevaluasi batch {i//batch_size + 1} ({len(batch_samples)} soal)...\")\n",
    "    result = evaluate_text_generation(\n",
    "        evaluation_dataset=dataset,\n",
    "        llm_model=llm_model,\n",
    "        embedding_model=embedding_model,\n",
    "        experiment_name=f\"eval_batch_answer_generation_{i//batch_size + 1}\"\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "    if i + batch_size < len(samples):\n",
    "        print(\"⏳ Menunggu 30 detik sebelum lanjut ke batch berikutnya...\\n\")\n",
    "        time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5b81d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] result.scores[0] = {'rouge1_precision(mode=precision)': 0.45, 'rouge1_recall(mode=recall)': 0.6923076923076923, 'rouge1_fmeasure(mode=fmeasure)': 0.5454545454545455, 'rougeL_precision(mode=precision)': 0.39, 'rougeL_recall(mode=recall)': 0.6, 'rougeL_fmeasure(mode=fmeasure)': 0.4727272727272727, 'answer_relevancy': 0.28667318828731536, 'faithfulness': 1.0}\n",
      "[DEBUG] type = <class 'dict'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] result.scores[0] = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr.scores[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] type = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(r.scores[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] keys = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# cukup 1x\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "for r in all_results:\n",
    "    print(f\"[DEBUG] result.scores[0] = {r.scores[0]}\")\n",
    "    print(f\"[DEBUG] type = {type(r.scores[0])}\")\n",
    "    print(f\"[DEBUG] keys = {vars(r.scores[0]).keys()}\")\n",
    "    break  # cukup 1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710727c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 RATA-RATA EVALUASI KESELURUHAN:\n",
      "rouge1_precision(mode=precision): 0.4095\n",
      "rouge1_recall(mode=recall): 0.8321\n",
      "rouge1_fmeasure(mode=fmeasure): 0.5402\n",
      "rougeL_precision(mode=precision): 0.3688\n",
      "rougeL_recall(mode=recall): 0.7577\n",
      "rougeL_fmeasure(mode=fmeasure): 0.4883\n",
      "answer_relevancy: 0.2630\n",
      "faithfulness: 1.0000\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_results.append(result)\n",
    "# === Rata-rata skor keseluruhan ===\n",
    "from collections import defaultdict\n",
    "\n",
    "# Kumpulkan semua skor berdasarkan metr\n",
    "# ik\n",
    "combined_scores = defaultdict(list)\n",
    "\n",
    "for result in all_results:\n",
    "    for batch_dict in result.scores:  # result.scores adalah list of dict\n",
    "        for metric_name, score in batch_dict.items():\n",
    "            combined_scores[metric_name].append(score)\n",
    "\n",
    "# Tampilkan rata-rata per metrik\n",
    "print(\"\\n📊 RATA-RATA EVALUASI KESELURUHAN:\")\n",
    "for metric, scores in combined_scores.items():\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"{metric}: {avg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
